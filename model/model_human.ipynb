{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchsummary\n",
    "from alive_progress import alive_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_dir = {\n",
    "    'machine': '../machine/mixed/',\n",
    "    'nature': '../nature/mixed/',\n",
    "    'human': '../human/mixed/'\n",
    "}\n",
    "\n",
    "clean_dir = {\n",
    "    'machine': '../machine/clean/',\n",
    "    'nature': '../nature/clean/',\n",
    "    'human': '../human/clean/'\n",
    "}\n",
    "\n",
    "# TODO\n",
    "test_dir = {\n",
    "    'input': '../classification_data/human_output/',\n",
    "    'output': '../test_output/human/',\n",
    "    'visual': '../visualization/human/'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_snr(clean_signal, denoised_signal):\n",
    "    \"\"\"\n",
    "    Calculate the Signal-to-Noise Ratio (SNR) in decibels.\n",
    "    \n",
    "    Args:\n",
    "    clean_signal (torch.Tensor): Ground truth clean signal, shape (1, 1, H, W)\n",
    "    denoised_signal (torch.Tensor): Model's output signal, shape (1, 1, H, W)\n",
    "    \n",
    "    Returns:\n",
    "    float: SNR value in decibels.\n",
    "    \"\"\"\n",
    "    assert clean_signal.shape == denoised_signal.shape, \"Shapes of input tensors must match.\"\n",
    "    clean_signal = clean_signal.squeeze()\n",
    "    denoised_signal = denoised_signal.squeeze()\n",
    "    signal_power = torch.mean(clean_signal ** 2)\n",
    "    noise_power = torch.mean((clean_signal - denoised_signal) ** 2)\n",
    "    if noise_power == 0:\n",
    "        return float('inf')\n",
    "    snr = 10 * torch.log10(signal_power / noise_power)\n",
    "    return snr.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MelSpectrogram參數\n",
    "n_mels = 128                # 保持 Mel 頻譜圖的解析度\n",
    "n_fft = 1024                # 提高 FFT 窗口大小以適配更多信號頻率\n",
    "hop_length = 512            # 保持 hop_length 為 n_fft 的一半\n",
    "win_length = 1024           # 窗口大小與 n_fft 保持一致（或設為 None 使用默認值）\n",
    "sample_rate = 16000         # 採樣率保持不變，適合語音處理\n",
    "f_max = sample_rate // 2    # 預設為 Nyquist 頻率，即 8000 Hz\n",
    "duration = 5                # 音頻時長為 5 秒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spectrogram_from_npy(mixed_dir, clean_dir = None):\n",
    "    \"\"\"Load mel spectrogram from a NumPy file.\"\"\"\n",
    "    mixed_mel_spectrograms = []\n",
    "    clean_mel_spectrograms = []\n",
    "    \n",
    "    length = len(os.listdir(mixed_dir))\n",
    "    \n",
    "    print(f\"Loading {length} files...\")\n",
    "\n",
    "    with alive_bar(length, force_tty=True) as bar:\n",
    "        for filename in sorted(os.listdir(mixed_dir)):\n",
    "            if \".gitkeep\" in filename:\n",
    "                continue\n",
    "            try:\n",
    "                # 使用完整路徑\n",
    "                mixed_path = os.path.join(mixed_dir, filename)\n",
    "                if (clean_dir != None): clean_path = os.path.join(clean_dir, filename)\n",
    "                \n",
    "                mixed_mel_spectrogram_db = np.load(mixed_path)\n",
    "                if (clean_dir != None): clean_mel_spectrogram_db = np.load(clean_path)\n",
    "\n",
    "                # # 轉換為 PyTorch tensor 並添加通道維度\n",
    "                mixed_mel_tensor = torch.tensor(mixed_mel_spectrogram_db, dtype=torch.float32).squeeze().unsqueeze(0)\n",
    "                if (clean_dir != None): clean_mel_tensor = torch.tensor(clean_mel_spectrogram_db, dtype=torch.float32).squeeze().unsqueeze(0)\n",
    "                \n",
    "                mixed_mel_spectrograms.append(mixed_mel_tensor)\n",
    "                if (clean_dir != None): clean_mel_spectrograms.append(clean_mel_tensor)\n",
    "                \n",
    "                bar()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error load file {filename}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    if (clean_dir != None): return mixed_mel_spectrograms, clean_mel_spectrograms\n",
    "    else: return mixed_mel_spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# machine_mixed_mel_spectrograms, machine_clean_mel_spectrograms = load_spectrogram_from_npy(mixed_dir['machine'], clean_dir['machine'])\n",
    "# nature_mixed_mel_spectrograms, nature_clean_mel_spectrograms = load_spectrogram_from_npy(mixed_dir['nature'], clean_dir['nature'])\n",
    "human_mixed_mel_spectrograms, human_clean_mel_spectrograms = load_spectrogram_from_npy(mixed_dir['human'], clean_dir['human'])\n",
    "test_mixed_mel_spectrograms = load_spectrogram_from_npy(test_dir['input'])\n",
    "mixed_mel_spectrograms = human_mixed_mel_spectrograms\n",
    "clean_mel_spectrograms = human_clean_mel_spectrograms\n",
    "target_dim = 160\n",
    "pad_width = [(0, 0), (0, 0), (0, target_dim - mixed_mel_spectrograms[0].shape[2])]\n",
    "mixed_mel_spectrograms = [np.pad(i, pad_width=pad_width, mode='constant', constant_values=0) for i in mixed_mel_spectrograms]\n",
    "clean_mel_spectrograms = [np.pad(i, pad_width=pad_width, mode='constant', constant_values=0) for i in clean_mel_spectrograms]\n",
    "test_mixed_mel_spectrograms = [np.pad(i, pad_width=pad_width, mode='constant', constant_values=0) for i in test_mixed_mel_spectrograms]\n",
    "mixed_mel_spectrograms_train, mixed_mel_spectrograms_val, clean_mel_spectrograms_train, clean_mel_spectrograms_val = train_test_split(mixed_mel_spectrograms, clean_mel_spectrograms, test_size=0.2, random_state=42)\n",
    "time_steps = mixed_mel_spectrograms[0].shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "img = librosa.display.specshow(mixed_mel_spectrograms_train[0][0], sr=sample_rate, x_axis='time', y_axis='mel', ax=ax)\n",
    "fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "ax.set(title='mixed_mel_spectrograms_train[0]')\n",
    "plt.show()\n",
    "\n",
    "# 將圖像保存至檔案\n",
    "output_path = test_dir['visual'] + \"mixed_mel_spectrograms_train[0].png\"  # 定義保存的檔案名稱和格式\n",
    "fig.savefig(output_path, dpi=300, bbox_inches='tight')  # dpi 決定解析度，bbox_inches='tight' 防止多餘空白\n",
    "plt.close(fig)  # 避免顯示圖像（若不需要可保留 plt.show()）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "img = librosa.display.specshow(clean_mel_spectrograms_train[0][0], sr=sample_rate, x_axis='time', y_axis='mel', ax=ax)\n",
    "fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "ax.set(title='clean_mel_spectrograms_train[0]')\n",
    "plt.show()\n",
    "\n",
    "# 將圖像保存至檔案\n",
    "output_path = test_dir['visual'] + \"clean_mel_spectrograms_train[0].png\"  # 定義保存的檔案名稱和格式\n",
    "fig.savefig(output_path, dpi=300, bbox_inches='tight')  # dpi 決定解析度，bbox_inches='tight' 防止多餘空白\n",
    "plt.close(fig)  # 避免顯示圖像（若不需要可保留 plt.show()）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoiseAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenoiseAutoencoder, self).__init__()\n",
    "        # 編碼器\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # 全連接層 ### shape 有問題\n",
    "        self.fc1 = nn.Linear(10240, 256)  # 假設輸入大小為 (1, 64, 64)\n",
    "        self.fc2 = nn.Linear(256, 10240)\n",
    "\n",
    "        # 解碼器\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 編碼器\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # 將特徵展平\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        # 全連接層處理\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # 恢復形狀為解碼器輸入\n",
    "        x = x.view(batch_size, 128, 8, 10)\n",
    "        # 解碼器\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model參數\n",
    "num_epochs = 100\n",
    "batch_size = 256\n",
    "learning_rate = 0.5\n",
    "lr_decay_step = 20\n",
    "lr_decay_gamma = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, mixed_data, clean_data):\n",
    "        self.mixed = mixed_data\n",
    "        self.clean = clean_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mixed)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.mixed[idx], self.clean[idx]\n",
    "\n",
    "dataset = AudioDataset(mixed_mel_spectrograms_train, clean_mel_spectrograms_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "dataloader_val = DataLoader(AudioDataset(mixed_mel_spectrograms_val, clean_mel_spectrograms_val), batch_size=len(mixed_mel_spectrograms_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "import gc\n",
    "model = DenoiseAutoencoder()\n",
    "\n",
    "# torchsummary.summary(model,(1,64,44))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=lr_decay_step, gamma=lr_decay_gamma)\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "training_snrs = []\n",
    "validation_snrs = []\n",
    "train_input = torch.tensor(np.array([mixed_mel_spectrograms_train[0]]), dtype=torch.float32)\n",
    "train_clean = torch.tensor(np.array([clean_mel_spectrograms_train[0]]), dtype=torch.float32)\n",
    "val_input = torch.tensor(np.array([mixed_mel_spectrograms_val[0]]), dtype=torch.float32)\n",
    "val_clean = torch.tensor(np.array([clean_mel_spectrograms_val[0]]), dtype=torch.float32)\n",
    "\n",
    "# 訓練過程\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    sum_loss = 0\n",
    "    for (mixed, clean) in dataloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 前向傳播\n",
    "        outputs = model(mixed)\n",
    "        loss = criterion(outputs, clean)\n",
    "        sum_loss += loss.item()\n",
    "            \n",
    "        # 反向傳播和優化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Step the scheduler to decay the learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # count training loss and SNR\n",
    "    snr = calculate_snr(clean, outputs)\n",
    "    training_losses.append(sum_loss / len(dataloader))\n",
    "    training_snrs.append(snr)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        fig, ax = plt.subplots()\n",
    "        img = librosa.display.specshow(outputs[0][0].detach().cpu().numpy(), sr=sample_rate, x_axis='time', y_axis='mel', ax=ax)\n",
    "        fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "        ax.set(title=f'output_mel_spectrograms_train[0]_epoch {epoch}')\n",
    "        plt.show()\n",
    "\n",
    "        # 將圖像保存至檔案\n",
    "        output_path = test_dir['visual'] + f'output_mel_spectrograms_train[0]_epoch {epoch}'  # 定義保存的檔案名稱和格式\n",
    "        fig.savefig(output_path, dpi=300, bbox_inches='tight')  # dpi 決定解析度，bbox_inches='tight' 防止多餘空白\n",
    "        plt.close(fig)  # 避免顯示圖像 (若不需要可保留 plt.show())\n",
    "        gc.collect()\n",
    "    \n",
    "    # count validation loss and SNR\n",
    "    val_output = model(val_input)\n",
    "    val_loss = criterion(val_output, val_clean)\n",
    "    val_snr = calculate_snr(val_clean, val_output)\n",
    "    validation_losses.append(val_loss.item())\n",
    "    validation_snrs.append(val_snr)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        fig, ax = plt.subplots()\n",
    "        img = librosa.display.specshow(val_output[0][0].detach().cpu().numpy(), sr=sample_rate, x_axis='time', y_axis='mel', ax=ax)\n",
    "        fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "        ax.set(title=f'output_mel_spectrograms_val[0]_epoch {epoch}')\n",
    "        plt.show()\n",
    "\n",
    "        # 將圖像保存至檔案\n",
    "        output_path = test_dir['visual'] + f'output_mel_spectrograms_val[0]_epoch {epoch}'  # 定義保存的檔案名稱和格式\n",
    "        fig.savefig(output_path, dpi=300, bbox_inches='tight')  # dpi 決定解析度，bbox_inches='tight' 防止多餘空白\n",
    "        plt.close(fig)  # 避免顯示圖像 (若不需要可保留 plt.show())\n",
    "        gc.collect()\n",
    "    \n",
    "    # Optionally, print the current learning rate and loss\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.12f}, Validation Loss: {val_loss.item():.12f}, Learning Rate: {current_lr:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training and validation losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(training_losses, label=\"Training Loss\")\n",
    "plt.plot(validation_losses, label=\"Validation Loss\")\n",
    "plt.title(\"Loss while training\")\n",
    "# plt.xlim([5, num_epochs])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 將圖像保存至檔案\n",
    "output_path = test_dir['visual'] + \"loss_while_training.png\"  # 定義保存的檔案名稱和格式\n",
    "fig.savefig(output_path, dpi=300, bbox_inches='tight')  # dpi 決定解析度，bbox_inches='tight' 防止多餘空白\n",
    "plt.close(fig)  # 避免顯示圖像（若不需要可保留 plt.show()）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the SNR of the first training sample\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_output = model(train_input)\n",
    "    snr = calculate_snr(train_clean, train_output)\n",
    "    print(f\"Training SNR:   {snr:.8f} dB\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_output = model(val_input)\n",
    "    snr = calculate_snr(val_clean, val_output)\n",
    "    print(f\"Validation SNR: {snr:.8f} dB\")\n",
    "    \n",
    "# plot the training and validation losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(training_snrs, label=\"Training SNR\")\n",
    "plt.plot(validation_snrs, label=\"Validation SNR\")\n",
    "plt.title(\"SNR while training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"dB\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 將圖像保存至檔案\n",
    "output_path = test_dir['visual'] + \"snr_while_training.png\"  # 定義保存的檔案名稱和格式\n",
    "fig.savefig(output_path, dpi=300, bbox_inches='tight')  # dpi 決定解析度，bbox_inches='tight' 防止多餘空白\n",
    "plt.close(fig)  # 避免顯示圖像（若不需要可保留 plt.show()）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "denoised_output = model(torch.from_numpy(mixed_mel_spectrograms_train[0]).unsqueeze(0))\n",
    "denoised_output = denoised_output[:, :, :, :157]\n",
    "fig, ax = plt.subplots()\n",
    "img = librosa.display.specshow(denoised_output.squeeze(0).squeeze(0).detach().numpy(), sr=sample_rate, x_axis='time', y_axis='mel', ax=ax)\n",
    "fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "ax.set(title='denoise_mel_spectrogram_train[0]')\n",
    "plt.show()\n",
    "\n",
    "# 將圖像保存至檔案\n",
    "output_path = test_dir['visual'] + \"denoise_mel_spectrogram_train[0].png\"  # 定義保存的檔案名稱和格式\n",
    "fig.savefig(output_path, dpi=300, bbox_inches='tight')  # dpi 決定解析度，bbox_inches='tight' 防止多餘空白\n",
    "plt.close(fig)  # 避免顯示圖像 (若不需要可保留 plt.show())\n",
    "\n",
    "denoised_output = model(torch.from_numpy(mixed_mel_spectrograms_val[0]).unsqueeze(0))\n",
    "denoised_output = denoised_output[:, :, :, :157]\n",
    "fig, ax = plt.subplots()\n",
    "img = librosa.display.specshow(denoised_output.squeeze(0).squeeze(0).detach().numpy(), sr=sample_rate, x_axis='time', y_axis='mel', ax=ax)\n",
    "fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "ax.set(title='denoise_mel_spectrogram_val[0]')\n",
    "plt.show()\n",
    "\n",
    "# 將圖像保存至檔案\n",
    "output_path = test_dir['visual'] + \"denoise_mel_spectrogram_val[0].png\"  # 定義保存的檔案名稱和格式\n",
    "fig.savefig(output_path, dpi=300, bbox_inches='tight')  # dpi 決定解析度，bbox_inches='tight' 防止多餘空白\n",
    "plt.close(fig)  # 避免顯示圖像 (若不需要可保留 plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"human_model.pth\") # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MelSpectrogram參數 (勿動)\n",
    "# n_mels = 128                # 保持 Mel 頻譜圖的解析度\n",
    "# n_fft = 1024                # 提高 FFT 窗口大小以適配更多信號頻率\n",
    "# hop_length = 512            # 保持 hop_length 為 n_fft 的一半\n",
    "# win_length = 1024           # 窗口大小與 n_fft 保持一致（或設為 None 使用默認值）\n",
    "# sample_rate = 16000         # 採樣率保持不變，適合語音處理\n",
    "# f_max = sample_rate // 2    # 預設為 Nyquist 頻率，即 8000 Hz\n",
    "# duration = 5                # 音頻時長為 5 秒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MIXED\n",
    "# mixed_waveform, sample_rate = librosa.load('test2.wav', sr=sample_rate)\n",
    "\n",
    "# # if sr != sample_rate:\n",
    "# #     mixed_waveform = librosa.resample(mixed_waveform, orig_sr=sr, target_sr=sample_rate)\n",
    "    \n",
    "# # cut to fit the duration\n",
    "# if len(mixed_waveform) > sample_rate * duration:\n",
    "#     mixed_waveform = mixed_waveform[:sample_rate * duration]\n",
    "\n",
    "# mixed_mel_spectrogram = librosa.feature.melspectrogram(\n",
    "#     y=mixed_waveform,\n",
    "#     sr=sample_rate,\n",
    "#     n_fft=n_fft,\n",
    "#     hop_length=hop_length,\n",
    "#     n_mels=n_mels\n",
    "# )\n",
    "\n",
    "# mixed_mel_spectrogram_db = librosa.power_to_db(\n",
    "#     mixed_mel_spectrogram, \n",
    "#     ref=np.max, \n",
    "#     amin=1e-10  # 避免log(0)\n",
    "# )\n",
    "\n",
    "# # mixed_mel_tensor = torch.tensor(mixed_mel_spectrogram, dtype=torch.float32).squeeze().unsqueeze(0)\n",
    "# mixed_mel_tensor = torch.tensor(mixed_mel_spectrogram_db, dtype=torch.float32).squeeze().unsqueeze(0)\n",
    "# # mixed_mel_tensor = torch.tensor(np.load('nature_mixed.npy'), dtype=torch.float32).squeeze().unsqueeze(0)\n",
    "\n",
    "# mixed_mel = mixed_mel_tensor\n",
    "# mixed_output = mixed_mel.squeeze(0).squeeze(0).detach().numpy()\n",
    "# mixed_output = librosa.db_to_power(mixed_output)\n",
    "\n",
    "# audio_signal = librosa.feature.inverse.mel_to_audio(\n",
    "#     mixed_output,\n",
    "#     sr=sample_rate,\n",
    "#     n_fft=n_fft,\n",
    "#     hop_length=hop_length,\n",
    "#     n_iter=512\n",
    "# )\n",
    "\n",
    "# audio_signal = audio_signal / np.max(np.abs(audio_signal))\n",
    "\n",
    "\n",
    "# librosa.display.waveshow(audio_signal, sr=sample_rate)\n",
    "# soundfile.write('test_librosa_mixed.wav', audio_signal, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DENOISED\n",
    "# model.eval()  # 设置模型为评估模式\n",
    "# # print(mixed_mel_tensor.shape)\n",
    "# print(test_mixed_mel_spectrograms[0].shape)\n",
    "# mixed_mel_np_array = np.array(test_mixed_mel_spectrograms)\n",
    "# for i in range(len(mixed_mel_np_array)):\n",
    "#     mixed_mel_np_array[i] = mixed_mel_np_array[i][:, :, :157]\n",
    "# print(mixed_mel_np_array.shape)\n",
    "\n",
    "# mixed_mel_tensor = torch.tensor(mixed_mel_np_array, dtype=torch.float32)\n",
    "\n",
    "# denoised_output = model(mixed_mel_tensor)\n",
    "# denoised_output = denoised_output[:, :, :, :157]\n",
    "# print(denoised_output.shape)\n",
    "# denoised_output = denoised_output.squeeze(0).squeeze(0).detach().numpy()\n",
    "# denoised_output = librosa.db_to_power(denoised_output)\n",
    "\n",
    "# audio_signal = librosa.feature.inverse.mel_to_audio(denoised_output, sr=sample_rate, n_iter=500)\n",
    "# audio_signal = audio_signal / np.max(np.abs(audio_signal))\n",
    "\n",
    "# librosa.display.waveshow(audio_signal, sr=sample_rate)\n",
    "# soundfile.write('test_librosa_denoised.wav', audio_signal, sample_rate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
