{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as functional\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(base_dirs, stack):\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # 遍歷每個目錄\n",
    "    for label, directory in enumerate(base_dirs):\n",
    "        dir_path = Path(directory)\n",
    "        print(f\"Processing directory: {dir_path} (Label: {label})\")\n",
    "        \n",
    "        # 取得目錄下所有的 .npy 檔案\n",
    "        npy_files = list(dir_path.glob(\"*.npy\"))\n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "        for npy_file in npy_files:\n",
    "            try:\n",
    "                # 載入 npy 檔案\n",
    "                data = np.load(npy_file)\n",
    "                \n",
    "                # 檢查數據是否是 2D\n",
    "                if len(data.shape) != 2:\n",
    "                    raise ValueError(f\"Expected 2D data, got shape {data.shape}\")\n",
    "                if data.shape[1] != 157:\n",
    "                    data = data[:, 0:157]\n",
    "                # 將數據和標籤加入列表\n",
    "                all_data.append(data)\n",
    "                all_labels.append(label)  # 每個檔案對應一個標籤\n",
    "                \n",
    "                # print(f\"Loaded {npy_file.name}: Shape {data.shape}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {npy_file}: {str(e)}\")\n",
    "\n",
    "        \n",
    "        all_data = np.stack(all_data, axis=0)  # (N, H, W)\n",
    "        all_labels = np.array(all_labels)  # (N,)\n",
    "    \n",
    "        all_data = np.expand_dims(all_data, axis=1)\n",
    "    # 正規化數據到 [0, 1] 區間\n",
    "    # all_data = all_data.astype(np.float32) / 255.0\n",
    "    print(f\"\\nFinal dataset shape: {all_data.shape}\")\n",
    "    print(f\"Labels shape: {all_labels.shape}\")\n",
    "    print(f\"Unique labels: {np.unique(all_labels)}\")\n",
    "    \n",
    "    return all_data, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelClassifier(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(MelClassifier, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = None  # 延遲初始化\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        if self.classifier is None:  # 第一次執行 forward 時初始化\n",
    "            num_features = x.view(x.size(0), -1).size(1)\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(num_features, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(512, 3)\n",
    "            ).to(x.device)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.005\n",
    "num_epochs = 20\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        # print(f\"Model output shape: {outputs.shape}\")\n",
    "        # print(f\"Labels shape: {labels.shape}\")\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(train_loader), 100. * correct / total\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(test_loader), 100. * correct / total\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_classified_results_npy(model, test_data, output_dirs, device):\n",
    "    \"\"\"\n",
    "    保存分類結果為 .npy 檔案到不同資料夾\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    os.makedirs(output_dirs[0], exist_ok=True)  # human_output\n",
    "    os.makedirs(output_dirs[1], exist_ok=True)  # machine_output\n",
    "    os.makedirs(output_dirs[2], exist_ok=True)  # nature_output\n",
    "    \n",
    "    for idx, sample in enumerate(test_data):\n",
    "            # 增加 batch 和通道維度 (1, C, H, W)\n",
    "            # sample = np.expand_dims(sample, axis=0)  # (1, H, W)\n",
    "            sample = np.expand_dims(sample, axis=1)  # (1, 1, H, W)\n",
    "            print(sample.shape)\n",
    "            sample = torch.FloatTensor(sample).to(device)\n",
    "            \n",
    "            # 推理\n",
    "            output = model(sample)\n",
    "            predicted = output.argmax(dim=1).item()\n",
    "            \n",
    "            # 保存結果\n",
    "            output_dir = output_dirs[predicted]\n",
    "            Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "            np.save(f\"{output_dir}/sample_{idx}.npy\", sample.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: ..\\human\\mixed (Label: 0)\n",
      "Processing directory: ..\\machine\\mixed (Label: 1)\n",
      "Processing directory: ..\\nature\\mixed (Label: 2)\n",
      "\n",
      "Final dataset shape: (7500, 1, 128, 157)\n",
      "Labels shape: (7500,)\n",
      "Unique labels: [2]\n"
     ]
    }
   ],
   "source": [
    "# label of each directory: 0, 1, 2\n",
    "base_dirs = [\"../human/mixed\", \"../machine/mixed\", \"../nature/mixed\"]\n",
    "\n",
    "# load data\n",
    "data, labels = load_data(base_dirs, True)\n",
    "\n",
    "# train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "train_dataset = MelDataset(X_train, y_train)\n",
    "test_dataset = MelDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train_image, labels in train_loader:\n",
    "#     print(train_image.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]\n",
      "Train Loss: 0.0290, Train Acc: 99.28%\n",
      "Test Loss: 0.0000, Test Acc: 100.00%\n",
      "Epoch [2/20]\n",
      "Train Loss: 0.0124, Train Acc: 99.97%\n",
      "Test Loss: 0.0000, Test Acc: 100.00%\n",
      "Epoch [3/20]\n",
      "Train Loss: 0.0000, Train Acc: 100.00%\n",
      "Test Loss: 0.0000, Test Acc: 100.00%\n",
      "Epoch [4/20]\n",
      "Train Loss: 0.0000, Train Acc: 100.00%\n",
      "Test Loss: 0.0000, Test Acc: 100.00%\n",
      "Epoch [5/20]\n",
      "Train Loss: 0.0000, Train Acc: 100.00%\n",
      "Test Loss: 0.0000, Test Acc: 100.00%\n",
      "Epoch [6/20]\n",
      "Train Loss: 0.0000, Train Acc: 100.00%\n",
      "Test Loss: 0.0000, Test Acc: 100.00%\n",
      "Epoch [7/20]\n",
      "Train Loss: 0.0000, Train Acc: 100.00%\n",
      "Test Loss: 0.0000, Test Acc: 100.00%\n",
      "Epoch [8/20]\n",
      "Train Loss: 0.0000, Train Acc: 100.00%\n",
      "Test Loss: 0.0000, Test Acc: 100.00%\n",
      "Epoch [9/20]\n",
      "Train Loss: 0.0000, Train Acc: 100.00%\n",
      "Test Loss: 0.0000, Test Acc: 100.00%\n",
      "Epoch [10/20]\n",
      "Train Loss: 0.0000, Train Acc: 100.00%\n",
      "Test Loss: 0.0000, Test Acc: 100.00%\n",
      "Epoch [11/20]\n",
      "Train Loss: 0.0014, Train Acc: 99.98%\n",
      "Test Loss: 0.0000, Test Acc: 100.00%\n",
      "Epoch [12/20]\n",
      "Train Loss: 0.0000, Train Acc: 100.00%\n",
      "Test Loss: 0.0000, Test Acc: 100.00%\n",
      "Epoch [13/20]\n",
      "Train Loss: 0.0000, Train Acc: 100.00%\n",
      "Test Loss: 0.0000, Test Acc: 100.00%\n",
      "Epoch [14/20]\n",
      "Train Loss: 0.0000, Train Acc: 100.00%\n",
      "Test Loss: 0.0000, Test Acc: 100.00%\n",
      "Epoch [15/20]\n",
      "Train Loss: 0.0042, Train Acc: 99.98%\n",
      "Test Loss: 0.0000, Test Acc: 100.00%\n",
      "Epoch [16/20]\n",
      "Train Loss: 0.0004, Train Acc: 99.98%\n",
      "Test Loss: 0.0000, Test Acc: 100.00%\n",
      "Epoch [17/20]\n",
      "Train Loss: 0.0000, Train Acc: 100.00%\n",
      "Test Loss: 0.0000, Test Acc: 100.00%\n",
      "Epoch [18/20]\n",
      "Train Loss: 0.0000, Train Acc: 100.00%\n",
      "Test Loss: 0.0000, Test Acc: 100.00%\n",
      "Epoch [19/20]\n",
      "Train Loss: 0.0000, Train Acc: 100.00%\n",
      "Test Loss: 0.0000, Test Acc: 100.00%\n",
      "Epoch [20/20]\n",
      "Train Loss: 0.0002, Train Acc: 99.98%\n",
      "Test Loss: 0.0000, Test Acc: 100.00%\n",
      "\n",
      "Final Test Accuracy: 100.00%\n",
      "Final Test Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_channel = data.shape[1]\n",
    "model = MelClassifier(input_channel)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n",
    "        test_loss, test_acc = evaluate_model(model, test_loader, criterion, device)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "\n",
    "final_test_loss, final_test_acc = evaluate_model(model, test_loader, criterion, device)\n",
    "print(f\"\\nFinal Test Accuracy: {final_test_acc:.2f}%\")\n",
    "print(f\"Final Test Loss: {final_test_loss:.4f}\")\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: ..\\test_data (Label: 0)\n",
      "\n",
      "Final dataset shape: (105, 1, 128, 157)\n",
      "Labels shape: (105,)\n",
      "Unique labels: [0]\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n",
      "(1, 1, 128, 157)\n"
     ]
    }
   ],
   "source": [
    "# 保存測試分類結果\n",
    "test_data_dir = [\"../test_data\"]\n",
    "test_data, _ = load_data(test_data_dir, False)\n",
    "output_dirs = {0: \"human_output\", 1: \"machine_output\", 2: \"nature_output\"}\n",
    "save_classified_results_npy(model, test_data, output_dirs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"classify_model.pth\") # TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
