{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_dir = \"../mixed_data/\"\n",
    "clean_dir = \"../clean_data/\"\n",
    "nature_mixed_dir = \"../classified_sound_1115/nature/mixed/\"\n",
    "nature_clean_dir = \"../classified_sound_1115/nature/clean/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MelSpectrogram參數\n",
    "n_mels = 128                # 保持 Mel 頻譜圖的解析度\n",
    "n_fft = 1024                # 提高 FFT 窗口大小以適配更多信號頻率\n",
    "hop_length = 512            # 保持 hop_length 為 n_fft 的一半\n",
    "win_length = 1024           # 窗口大小與 n_fft 保持一致（或設為 None 使用默認值）\n",
    "sample_rate = 16000         # 採樣率保持不變，適合語音處理\n",
    "f_max = sample_rate // 2    # 預設為 Nyquist 頻率，即 8000 Hz\n",
    "duration = 5                # 音頻時長為 5 秒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from alive_progress import alive_bar\n",
    "import time\n",
    "\n",
    "# def save_spectrogram_as_npy(spectrogram, save_path):\n",
    "#     \"\"\"Save mel spectrogram as a NumPy array.\"\"\"\n",
    "#     np.save(save_path, spectrogram)  # Save as .npy file\n",
    "\n",
    "# def sound_to_spectrogram(mixed_dir, clean_dir, sample_rate, duration, n_mels):\n",
    "    \n",
    "#     length = len([f for f in os.listdir(mixed_dir) if f != \".gitkeep\"])\n",
    "    \n",
    "#     # print(f\"Loading {length} files...\")\n",
    "\n",
    "#     with alive_bar(length, force_tty=True) as bar:\n",
    "#         for filename in sorted(os.listdir(mixed_dir)):\n",
    "#             if \".gitkeep\" in filename:\n",
    "#                 continue\n",
    "#             try:\n",
    "#                 # 使用完整路徑\n",
    "#                 mixed_path = os.path.join(mixed_dir, filename)\n",
    "#                 clean_path = os.path.join(clean_dir, filename)\n",
    "                \n",
    "#                 # 使用 soundfile 替代 librosa.load\n",
    "#                 mixed_waveform, sr = sf.read(mixed_path)\n",
    "#                 clean_waveform, sr = sf.read(clean_path)\n",
    "                \n",
    "#                 # 如果採樣率不匹配，進行重採樣\n",
    "#                 if sr != sample_rate:\n",
    "#                     mixed_waveform = librosa.resample(mixed_waveform, orig_sr=sr, target_sr=sample_rate)\n",
    "#                     clean_waveform = librosa.resample(clean_waveform, orig_sr=sr, target_sr=sample_rate)\n",
    "                \n",
    "#                 # 如果指定了持續時間，裁剪音頻\n",
    "#                 if duration:\n",
    "#                     samples = int(duration * sample_rate)\n",
    "#                     mixed_waveform = mixed_waveform[:samples]\n",
    "#                     clean_waveform = clean_waveform[:samples]\n",
    "                \n",
    "#                 # 生成梅爾頻譜圖\n",
    "#                 mixed_mel_spectrogram = librosa.feature.melspectrogram(\n",
    "#                     y=mixed_waveform,\n",
    "#                     sr=sample_rate,\n",
    "#                     n_fft=n_fft,\n",
    "#                     hop_length=hop_length,\n",
    "#                     n_mels=n_mels\n",
    "#                 )\n",
    "#                 clean_mel_spectrogram = librosa.feature.melspectrogram(\n",
    "#                     y=clean_waveform,\n",
    "#                     sr=sample_rate,\n",
    "#                     n_fft=n_fft,\n",
    "#                     hop_length=hop_length,\n",
    "#                     n_mels=n_mels\n",
    "#                 )\n",
    "\n",
    "#                 # 轉換為分貝刻度\n",
    "#                 mixed_mel_spectrogram_db = librosa.power_to_db(\n",
    "#                     mixed_mel_spectrogram, \n",
    "#                     ref=np.max, \n",
    "#                     amin=1e-10  # 避免log(0)\n",
    "#                 )\n",
    "#                 clean_mel_spectrogram_db = librosa.power_to_db(\n",
    "#                     clean_mel_spectrogram, \n",
    "#                     ref=np.max, \n",
    "#                     amin=1e-10\n",
    "#                 )\n",
    "                \n",
    "#                 # Save spectrograms as .npy files\n",
    "#                 mixed_npy_path = os.path.join(\"../nature/mixed/\", f\"{filename[-4]}.npy\")\n",
    "#                 clean_npy_path = os.path.join(\"../nature/clean/\", f\"{filename[-4]}.npy\")\n",
    "\n",
    "#                 save_spectrogram_as_npy(mixed_mel_spectrogram_db, mixed_npy_path)\n",
    "#                 save_spectrogram_as_npy(clean_mel_spectrogram_db, clean_npy_path)\n",
    "                \n",
    "#                 bar()\n",
    "                \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing file {filename}: {str(e)}\")\n",
    "#                 continue\n",
    "            \n",
    "# sound_to_spectrogram(nature_mixed_dir, nature_clean_dir, sample_rate, duration, n_mels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spectrogram_from_npy(mixed_dir, clean_dir):\n",
    "    \"\"\"Load mel spectrogram from a NumPy file.\"\"\"\n",
    "    mixed_mel_spectrograms = []\n",
    "    clean_mel_spectrograms = []\n",
    "    \n",
    "    length = len(os.listdir(clean_dir))\n",
    "    \n",
    "    # print(f\"Loading {length} files...\")\n",
    "\n",
    "    with alive_bar(length, force_tty=True) as bar:\n",
    "        for filename in sorted(os.listdir(clean_dir)):\n",
    "            if \".gitkeep\" in filename:\n",
    "                continue\n",
    "            try:\n",
    "                # 使用完整路徑\n",
    "                mixed_path = os.path.join(mixed_dir, filename)\n",
    "                clean_path = os.path.join(clean_dir, filename)\n",
    "                \n",
    "                mixed_mel_spectrogram_db = np.load(mixed_path)\n",
    "                clean_mel_spectrogram_db = np.load(clean_path)\n",
    "\n",
    "                # # 轉換為 PyTorch tensor 並添加通道維度\n",
    "                mixed_mel_tensor = torch.tensor(mixed_mel_spectrogram_db, dtype=torch.float32).squeeze().unsqueeze(0)\n",
    "                clean_mel_tensor = torch.tensor(clean_mel_spectrogram_db, dtype=torch.float32).squeeze().unsqueeze(0)\n",
    "                print(mixed_mel_tensor.shape)\n",
    "                print(clean_mel_tensor.shape)\n",
    "                mixed_mel_spectrograms.append(mixed_mel_tensor)\n",
    "                clean_mel_spectrograms.append(clean_mel_tensor)\n",
    "                \n",
    "                bar()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error load file {filename}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    return mixed_mel_spectrograms, clean_mel_spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on 0: torch.Size([1, 80000, 128])                                               \n",
      "on 0: torch.Size([1, 128, 157])                                                 \n",
      "on 1: torch.Size([1, 80000, 128])                                               \n",
      "on 1: torch.Size([1, 128, 157])                                                 \n",
      "on 2: torch.Size([1, 80000, 128])                                               \n",
      "on 2: torch.Size([1, 128, 157])                                                 \n",
      "on 3: torch.Size([1, 80000, 128])                                               \n",
      "on 3: torch.Size([1, 128, 157])                                                 \n",
      "on 4: torch.Size([1, 80000, 128])                                               \n",
      "on 4: torch.Size([1, 128, 157])                                                 \n",
      "on 5: torch.Size([1, 80000, 128])                                               \n",
      "on 5: torch.Size([1, 128, 157])                                                 \n",
      "on 6: torch.Size([1, 80000, 128])                                               \n",
      "on 6: torch.Size([1, 128, 157])                                                 \n",
      "on 7: torch.Size([1, 80000, 128])                                               \n",
      "on 7: torch.Size([1, 128, 157])                                                 \n",
      "on 8: torch.Size([1, 80000, 128])                                               \n",
      "on 8: torch.Size([1, 128, 157])                                                 \n",
      "on 9: torch.Size([1, 80000, 128])                                               \n",
      "on 9: torch.Size([1, 128, 157])                                                 \n",
      "on 10: torch.Size([1, 80000, 128])                                              \n",
      "on 10: torch.Size([1, 128, 157])                                                \n",
      "on 11: torch.Size([1, 80000, 128])                                              \n",
      "on 11: torch.Size([1, 128, 157])                                                \n",
      "on 12: torch.Size([1, 80000, 128])                                              \n",
      "on 12: torch.Size([1, 128, 157])                                                \n",
      "on 13: torch.Size([1, 80000, 128])                                              \n",
      "on 13: torch.Size([1, 128, 157])                                                \n",
      "on 14: torch.Size([1, 80000, 128])                                              \n",
      "on 14: torch.Size([1, 128, 157])                                                \n",
      "on 15: torch.Size([1, 80000, 128])                                              \n",
      "on 15: torch.Size([1, 128, 157])                                                \n",
      "on 16: torch.Size([1, 80000, 128])                                              \n",
      "on 16: torch.Size([1, 128, 157])                                                \n",
      "on 17: torch.Size([1, 80000, 128])                                              \n",
      "on 17: torch.Size([1, 128, 157])                                                \n",
      "on 18: torch.Size([1, 80000, 128])                                              \n",
      "on 18: torch.Size([1, 128, 157])                                                \n",
      "on 19: torch.Size([1, 80000, 128])                                              \n",
      "on 19: torch.Size([1, 128, 157])                                                \n",
      "|████⚠︎                                   | (!) 20/200 [10%] in 1.1s (19.85/s)   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[168], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mixed_mel_spectrograms, clean_mel_spectrograms \u001b[38;5;241m=\u001b[39m \u001b[43mload_spectrogram_from_npy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../nature/mixed/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../nature/clean/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m mixed_mel_spectrograms_train, mixed_mel_spectrograms_val, clean_mel_spectrograms_train, clean_mel_spectrograms_val \u001b[38;5;241m=\u001b[39m train_test_split(mixed_mel_spectrograms, clean_mel_spectrograms, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      3\u001b[0m time_steps \u001b[38;5;241m=\u001b[39m mixed_mel_spectrograms[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n",
      "Cell \u001b[1;32mIn[167], line 20\u001b[0m, in \u001b[0;36mload_spectrogram_from_npy\u001b[1;34m(mixed_dir, clean_dir)\u001b[0m\n\u001b[0;32m     17\u001b[0m clean_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(clean_dir, filename)\n\u001b[0;32m     19\u001b[0m mixed_mel_spectrogram_db \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(mixed_path)\n\u001b[1;32m---> 20\u001b[0m clean_mel_spectrogram_db \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# # 轉換為 PyTorch tensor 並添加通道維度\u001b[39;00m\n\u001b[0;32m     23\u001b[0m mixed_mel_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(mixed_mel_spectrogram_db, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\numpy\\lib\\_npyio_impl.py:455\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    453\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 455\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    456\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mixed_mel_spectrograms, clean_mel_spectrograms = load_spectrogram_from_npy(\"../nature/mixed/\", \"../nature/clean/\")\n",
    "mixed_mel_spectrograms_train, mixed_mel_spectrograms_val, clean_mel_spectrograms_train, clean_mel_spectrograms_val = train_test_split(mixed_mel_spectrograms, clean_mel_spectrograms, test_size=0.2, random_state=42)\n",
    "time_steps = mixed_mel_spectrograms[0].shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoiseAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenoiseAutoencoder, self).__init__()\n",
    "        # 編碼器\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # 解碼器\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        # Crop to match the exact target size if needed\n",
    "        if x.size(-1) > time_steps:\n",
    "            x = x[..., :time_steps]  # Crop time steps to match target (batch_size, 1, 128, 54)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model參數\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "learning_rate = 0.1\n",
    "lr_decay_step = 40\n",
    "lr_decay_gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, mixed_data, clean_data):\n",
    "        self.mixed = mixed_data\n",
    "        self.clean = clean_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mixed)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.mixed[idx], self.clean[idx]\n",
    "\n",
    "dataset = AudioDataset(mixed_mel_spectrograms_train, clean_mel_spectrograms_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 32, 22]             160\n",
      "       BatchNorm2d-2           [-1, 16, 32, 22]              32\n",
      "              ReLU-3           [-1, 16, 32, 22]               0\n",
      "            Conv2d-4           [-1, 32, 16, 11]           4,640\n",
      "       BatchNorm2d-5           [-1, 32, 16, 11]              64\n",
      "              ReLU-6           [-1, 32, 16, 11]               0\n",
      "            Conv2d-7             [-1, 64, 8, 6]          18,496\n",
      "       BatchNorm2d-8             [-1, 64, 8, 6]             128\n",
      "              ReLU-9             [-1, 64, 8, 6]               0\n",
      "           Conv2d-10            [-1, 128, 4, 3]          73,856\n",
      "      BatchNorm2d-11            [-1, 128, 4, 3]             256\n",
      "             ReLU-12            [-1, 128, 4, 3]               0\n",
      "  ConvTranspose2d-13             [-1, 64, 8, 6]          73,792\n",
      "      BatchNorm2d-14             [-1, 64, 8, 6]             128\n",
      "             ReLU-15             [-1, 64, 8, 6]               0\n",
      "  ConvTranspose2d-16           [-1, 32, 16, 12]          18,464\n",
      "      BatchNorm2d-17           [-1, 32, 16, 12]              64\n",
      "             ReLU-18           [-1, 32, 16, 12]               0\n",
      "  ConvTranspose2d-19           [-1, 16, 32, 24]           4,624\n",
      "      BatchNorm2d-20           [-1, 16, 32, 24]              32\n",
      "             ReLU-21           [-1, 16, 32, 24]               0\n",
      "  ConvTranspose2d-22            [-1, 1, 64, 48]             145\n",
      "      BatchNorm2d-23            [-1, 1, 64, 48]               2\n",
      "================================================================\n",
      "Total params: 194,883\n",
      "Trainable params: 194,883\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.03\n",
      "Params size (MB): 0.74\n",
      "Estimated Total Size (MB): 1.79\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liuut\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([32, 1, 128, 157])) that is different to the input size (torch.Size([32, 1, 80000, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (157) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[150], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 前向傳播\u001b[39;00m\n\u001b[0;32m     20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(mixed)\n\u001b[1;32m---> 21\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# 反向傳播和優化\u001b[39;00m\n\u001b[0;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\loss.py:608\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\functional.py:3791\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3789\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3791\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(\n\u001b[0;32m   3793\u001b[0m     expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3794\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (157) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "model = DenoiseAutoencoder()\n",
    "# example_input = torch.randn(1, 1, 128, time_steps)  # Batch size = 1, Channels = 1\n",
    "# output = model(example_input)\n",
    "# print(\"Output shape:\", output.shape)  # Should be (1, 1, 128, time_steps)\n",
    "\n",
    "torchsummary.summary(model,(1,64,44))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=lr_decay_step, gamma=lr_decay_gamma)\n",
    "\n",
    "# 訓練過程\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for (mixed, clean) in dataloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 前向傳播\n",
    "        outputs = model(mixed)\n",
    "        loss = criterion(outputs, clean)\n",
    "            \n",
    "        # 反向傳播和優化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Step the scheduler to decay the learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Optionally, print the current learning rate and loss\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.12f}, Learning Rate: {current_lr:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfilename = \"1034-121119-0049.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAN\n",
    "waveform, sample_rate = librosa.load(''.join([clean_dir, testfilename]))\n",
    "\n",
    "clean_mel = clean_mel_spectrograms[0]\n",
    "clean_output = clean_mel.squeeze(0).squeeze(0).detach().numpy()\n",
    "clean_output = librosa.db_to_power(clean_output)\n",
    "\n",
    "audio_signal = librosa.feature.inverse.mel_to_audio(clean_output, sr=sample_rate, n_iter=500)\n",
    "audio_signal = audio_signal / np.max(np.abs(audio_signal))\n",
    "\n",
    "\n",
    "librosa.display.waveshow(audio_signal, sr=sample_rate)\n",
    "soundfile.write('test_librosa_clean.wav', audio_signal, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIXED\n",
    "waveform, sample_rate = librosa.load(''.join([mixed_dir, testfilename]))\n",
    "\n",
    "mixed_mel = mixed_mel_spectrograms[0]\n",
    "mixed_output = mixed_mel.squeeze(0).squeeze(0).detach().numpy()\n",
    "mixed_output = librosa.db_to_power(mixed_output)\n",
    "\n",
    "audio_signal = librosa.feature.inverse.mel_to_audio(mixed_output, sr=sample_rate, n_iter=500)\n",
    "audio_signal = audio_signal / np.max(np.abs(audio_signal))\n",
    "\n",
    "\n",
    "librosa.display.waveshow(audio_signal, sr=sample_rate)\n",
    "soundfile.write('test_librosa_mixed.wav', audio_signal, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DENOISED\n",
    "model.eval()  # 设置模型为评估模式\n",
    "\n",
    "denoised_output = model(mixed_mel_spectrograms[0].unsqueeze(0))\n",
    "denoised_output = denoised_output.squeeze(0).squeeze(0).detach().numpy()\n",
    "denoised_output = librosa.db_to_power(denoised_output)\n",
    "\n",
    "audio_signal = librosa.feature.inverse.mel_to_audio(denoised_output, sr=sample_rate, n_iter=500)\n",
    "audio_signal = audio_signal / np.max(np.abs(audio_signal))\n",
    "\n",
    "librosa.display.waveshow(audio_signal, sr=sample_rate)\n",
    "soundfile.write('test_librosa_denoised.wav', audio_signal, sample_rate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
