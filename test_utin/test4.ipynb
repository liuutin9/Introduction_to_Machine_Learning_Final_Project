{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchsummary\n",
    "from alive_progress import alive_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixed_dir = \"../mixed_data/\"\n",
    "# clean_dir = \"../clean_data/\"\n",
    "# nature_mixed_dir = \"../classified_sound_1115/nature/mixed/\"\n",
    "# nature_clean_dir = \"../classified_sound_1115/nature/clean/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MelSpectrogram參數\n",
    "n_mels = 128                # 保持 Mel 頻譜圖的解析度\n",
    "n_fft = 1024                # 提高 FFT 窗口大小以適配更多信號頻率\n",
    "hop_length = 512            # 保持 hop_length 為 n_fft 的一半\n",
    "win_length = 1024           # 窗口大小與 n_fft 保持一致（或設為 None 使用默認值）\n",
    "sample_rate = 16000         # 採樣率保持不變，適合語音處理\n",
    "f_max = sample_rate // 2    # 預設為 Nyquist 頻率，即 8000 Hz\n",
    "duration = 5                # 音頻時長為 5 秒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spectrogram_from_npy(mixed_dir, clean_dir):\n",
    "    \"\"\"Load mel spectrogram from a NumPy file.\"\"\"\n",
    "    mixed_mel_spectrograms = []\n",
    "    clean_mel_spectrograms = []\n",
    "    \n",
    "    length = len(os.listdir(clean_dir))\n",
    "    \n",
    "    # print(f\"Loading {length} files...\")\n",
    "\n",
    "    with alive_bar(length, force_tty=True) as bar:\n",
    "        for filename in sorted(os.listdir(clean_dir)):\n",
    "            if \".gitkeep\" in filename:\n",
    "                continue\n",
    "            try:\n",
    "                # 使用完整路徑\n",
    "                mixed_path = os.path.join(mixed_dir, filename)\n",
    "                clean_path = os.path.join(clean_dir, filename)\n",
    "                \n",
    "                mixed_mel_spectrogram_db = np.load(mixed_path)\n",
    "                clean_mel_spectrogram_db = np.load(clean_path)\n",
    "\n",
    "                # # 轉換為 PyTorch tensor 並添加通道維度\n",
    "                mixed_mel_tensor = torch.tensor(mixed_mel_spectrogram_db, dtype=torch.float32).squeeze().unsqueeze(0)\n",
    "                clean_mel_tensor = torch.tensor(clean_mel_spectrogram_db, dtype=torch.float32).squeeze().unsqueeze(0)\n",
    "                \n",
    "                mixed_mel_spectrograms.append(mixed_mel_tensor)\n",
    "                clean_mel_spectrograms.append(clean_mel_tensor)\n",
    "                \n",
    "                bar()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error load file {filename}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    return mixed_mel_spectrograms, clean_mel_spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|████████████████████████████████████████| 7500/7500 [100%] in 1:16.2 (98.46/s)  ▆█▆ 2131/7500 [28%] in 22s (~56s, 96. ▇▇▅ 2418/7500 [32%] in 25s (~52s, 97. ▇▅▃ 2947/7500 [39%] in 30s (~47s, 97. ▄▂▂ 2965/7500 [40%] in 30s (~46s, 97. ▂▂▄ 3065/7500 [41%] in 31s (~45s, 97. ▂▂▄ 3630/7500 [48%] in 37s (~40s, 97. ▂▂▄ 4995/7500 [67%] in 50s (~25s, 99.\n",
      "|████████████████████████████████████████| 7500/7500 [100%] in 43.3s (173.25/s)  ▃▁▃ 804/7500 [11%] in 3s (~28s, 252.9 ▂▄▆ 1333/7500 [18%] in 4s (~18s, 347. ▅▇▇ 1442/7500 [19%] in 4s (~17s, 362. ▆█▆ 1938/7500 [26%] in 5s (~14s, 416. ▄▆█ 2373/7500 [32%] in 5s (~11s, 455. ▃▅▇ 2802/7500 [37%] in 6s (~10s, 483. ▅▇▇ 2839/7500 [38%] in 6s (~10s, 486. █▆▄ 2944/7500 [39%] in 6s (~9s, 490.2 ▃▅▇ 3257/7500 [43%] in 6s (~8s, 508.4 █▆▄ 3409/7500 [45%] in 7s (~8s, 514.4 ▃▁▃ 4446/7500 [59%] in 16s (~11s, 279 ▆▄▂ 4734/7500 [63%] in 18s (~11s, 257 ▄▂▂ 4896/7500 [65%] in 20s (~11s, 247 ▄▆█ 4989/7500 [67%] in 21s (~10s, 240 ▂▂▄ 5399/7500 [72%] in 25s (~10s, 220 ▃▁▃ 5628/7500 [75%] in 26s (~9s, 213. ▂▂▄ 5941/7500 [79%] in 29s (~8s, 204. █▆▄ 5983/7500 [80%] in 29s (~7s, 203. ▃▅▇ 6029/7500 [80%] in 30s (~7s, 202. ▂▂▄ 7111/7500 [95%] in 40s (~2s, 179.\n",
      "|████████████████████████████████████████| 7500/7500 [100%] in 11.0s (683.02/s)  ▂▂▄ 822/7500 [11%] in 1s (~11s, 632.5 ▂▄▆ 887/7500 [12%] in 1s (~11s, 631.0 █▆▄ 1103/7500 [15%] in 2s (~10s, 645. ▇▅▃ 3382/7500 [45%] in 5s (~6s, 677.6 ▂▄▆ 4457/7500 [59%] in 7s (~5s, 677.5 ▇▇▅ 4630/7500 [62%] in 7s (~4s, 675.4 ▇▅▃ 5665/7500 [76%] in 8s (~3s, 687.2 ▆█▆ 6848/7500 [91%] in 10s (~1s, 680.\n"
     ]
    }
   ],
   "source": [
    "machine_mixed_mel_spectrograms, machine_clean_mel_spectrograms = load_spectrogram_from_npy(\"../machine/mixed/\", \"../machine/clean/\")\n",
    "nature_mixed_mel_spectrograms, nature_clean_mel_spectrograms = load_spectrogram_from_npy(\"../nature/mixed/\", \"../nature/clean/\")\n",
    "human_mixed_mel_spectrograms, human_clean_mel_spectrograms = load_spectrogram_from_npy(\"../human/mixed/\", \"../human/clean/\")\n",
    "mixed_mel_spectrograms = machine_mixed_mel_spectrograms + nature_mixed_mel_spectrograms + human_mixed_mel_spectrograms\n",
    "clean_mel_spectrograms = machine_clean_mel_spectrograms + nature_clean_mel_spectrograms + human_clean_mel_spectrograms\n",
    "mixed_mel_spectrograms_train, mixed_mel_spectrograms_val, clean_mel_spectrograms_train, clean_mel_spectrograms_val = train_test_split(mixed_mel_spectrograms, clean_mel_spectrograms, test_size=0.2, random_state=42)\n",
    "time_steps = mixed_mel_spectrograms[0].shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape = torch.Size([1, 128, 157])\n"
     ]
    }
   ],
   "source": [
    "print(f'X shape = {mixed_mel_spectrograms[0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoiseAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenoiseAutoencoder, self).__init__()\n",
    "        # 編碼器\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # 全連接層 ### shape 有問題\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 256)  # 假設輸入大小為 (1, 64, 64)\n",
    "        self.fc2 = nn.Linear(256, 128 * 4 * 4)\n",
    "\n",
    "        # 解碼器\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 編碼器\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # 將特徵展平\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        # 全連接層處理\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # 恢復形狀為解碼器輸入\n",
    "        x = x.view(batch_size, 128, 4, 4)\n",
    "        \n",
    "        # 解碼器\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model參數\n",
    "num_epochs = 100\n",
    "batch_size = 256\n",
    "learning_rate = 0.5\n",
    "lr_decay_step = 20\n",
    "lr_decay_gamma = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, mixed_data, clean_data):\n",
    "        self.mixed = mixed_data\n",
    "        self.clean = clean_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mixed)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.mixed[idx], self.clean[idx]\n",
    "\n",
    "dataset = AudioDataset(mixed_mel_spectrograms_train, clean_mel_spectrograms_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "dataloader_val = DataLoader(AudioDataset(mixed_mel_spectrograms_val, clean_mel_spectrograms_val), batch_size=len(mixed_mel_spectrograms_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[256, 128, 4, 4]' is invalid for input of size 2621440",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 前向傳播\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmixed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, clean)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# 反向傳播和優化\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Utin Liu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Utin Liu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[17], line 52\u001b[0m, in \u001b[0;36mDenoiseAutoencoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     49\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# 恢復形狀為解碼器輸入\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# 解碼器\u001b[39;00m\n\u001b[0;32m     55\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[256, 128, 4, 4]' is invalid for input of size 2621440"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "model = DenoiseAutoencoder()\n",
    "# example_input = torch.randn(1, 1, 128, time_steps)  # Batch size = 1, Channels = 1\n",
    "# output = model(example_input)\n",
    "# print(\"Output shape:\", output.shape)  # Should be (1, 1, 128, time_steps)\n",
    "\n",
    "# torchsummary.summary(model,(1,64,44))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=lr_decay_step, gamma=lr_decay_gamma)\n",
    "training_losses = []\n",
    "valdation_losses = []\n",
    "\n",
    "# 訓練過程\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for (mixed, clean) in dataloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 前向傳播\n",
    "        outputs = model(mixed)\n",
    "        loss = criterion(outputs, clean)\n",
    "            \n",
    "        # 反向傳播和優化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Step the scheduler to decay the learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # count validation loss\n",
    "    val_output = model(next(iter(dataloader_val))[0])\n",
    "    val_loss = criterion(val_output, next(iter(dataloader_val))[1])\n",
    "    valdation_losses.append(val_loss.item())\n",
    "    \n",
    "    training_losses.append(loss.item())\n",
    "    # Optionally, print the current learning rate and loss\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.12f}, Validation Loss: {val_loss.item():.12f}, Learning Rate: {current_lr:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training and validation losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(training_losses, label=\"Training Loss\")\n",
    "plt.plot(valdation_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MelSpectrogram參數 (勿動)\n",
    "n_mels = 128                # 保持 Mel 頻譜圖的解析度\n",
    "n_fft = 1024                # 提高 FFT 窗口大小以適配更多信號頻率\n",
    "hop_length = 512            # 保持 hop_length 為 n_fft 的一半\n",
    "win_length = 1024           # 窗口大小與 n_fft 保持一致（或設為 None 使用默認值）\n",
    "sample_rate = 16000         # 採樣率保持不變，適合語音處理\n",
    "f_max = sample_rate // 2    # 預設為 Nyquist 頻率，即 8000 Hz\n",
    "duration = 5                # 音頻時長為 5 秒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CLEAN\n",
    "# clean_waveform, sr = librosa.load('test_clean.wav', sr=sample_rate)\n",
    "\n",
    "# if sr != sample_rate:\n",
    "#     clean_waveform = librosa.resample(clean_waveform, orig_sr=sr, target_sr=sample_rate)\n",
    "\n",
    "# clean_mel_spectrogram = librosa.feature.melspectrogram(\n",
    "#     y=clean_waveform,\n",
    "#     sr=sample_rate,\n",
    "#     n_fft=n_fft,\n",
    "#     hop_length=hop_length,\n",
    "#     n_mels=n_mels\n",
    "# )\n",
    "\n",
    "# clean_mel_spectrogram_db = librosa.power_to_db(\n",
    "#     clean_mel_spectrogram, \n",
    "#     ref=np.max, \n",
    "#     amin=1e-10  # 避免log(0)\n",
    "# )\n",
    "\n",
    "# clean_mel_tensor = torch.tensor(clean_mel_spectrogram_db, dtype=torch.float32).squeeze().unsqueeze(0)\n",
    "\n",
    "# clean_mel = clean_mel_tensor\n",
    "# clean_output = clean_mel.squeeze(0).squeeze(0).detach().numpy()\n",
    "# clean_output = librosa.db_to_power(clean_output)\n",
    "\n",
    "# audio_signal = librosa.feature.inverse.mel_to_audio(clean_output, sr=sample_rate, n_iter=500)\n",
    "# audio_signal = audio_signal / np.max(np.abs(audio_signal))\n",
    "\n",
    "\n",
    "# librosa.display.waveshow(audio_signal, sr=sample_rate)\n",
    "# soundfile.write('test_librosa_clean.wav', audio_signal, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIXED\n",
    "mixed_waveform, sample_rate = librosa.load('test2.wav', sr=sample_rate)\n",
    "\n",
    "# if sr != sample_rate:\n",
    "#     mixed_waveform = librosa.resample(mixed_waveform, orig_sr=sr, target_sr=sample_rate)\n",
    "    \n",
    "# cut to fit the duration\n",
    "# if len(mixed_waveform) > sample_rate * duration:\n",
    "#     mixed_waveform = mixed_waveform[:sample_rate * duration]\n",
    "\n",
    "mixed_mel_spectrogram = librosa.feature.melspectrogram(\n",
    "    y=mixed_waveform,\n",
    "    sr=sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    hop_length=hop_length,\n",
    "    n_mels=n_mels\n",
    ")\n",
    "\n",
    "mixed_mel_spectrogram_db = librosa.power_to_db(\n",
    "    mixed_mel_spectrogram, \n",
    "    ref=np.max, \n",
    "    amin=1e-10  # 避免log(0)\n",
    ")\n",
    "\n",
    "# mixed_mel_tensor = torch.tensor(mixed_mel_spectrogram, dtype=torch.float32).squeeze().unsqueeze(0)\n",
    "mixed_mel_tensor = torch.tensor(mixed_mel_spectrogram_db, dtype=torch.float32).squeeze().unsqueeze(0)\n",
    "# mixed_mel_tensor = torch.tensor(np.load('nature_mixed.npy'), dtype=torch.float32).squeeze().unsqueeze(0)\n",
    "\n",
    "mixed_mel = mixed_mel_tensor\n",
    "mixed_output = mixed_mel.squeeze(0).squeeze(0).detach().numpy()\n",
    "mixed_output = librosa.db_to_power(mixed_output)\n",
    "\n",
    "audio_signal = librosa.feature.inverse.mel_to_audio(\n",
    "    mixed_output,\n",
    "    sr=sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    hop_length=hop_length,\n",
    "    n_iter=4096\n",
    ")\n",
    "\n",
    "audio_signal = audio_signal / np.max(np.abs(audio_signal))\n",
    "\n",
    "\n",
    "librosa.display.waveshow(audio_signal, sr=sample_rate)\n",
    "soundfile.write('test_librosa_mixed.wav', audio_signal, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DENOISED\n",
    "model.eval()  # 设置模型为评估模式\n",
    "\n",
    "denoised_output = model(mixed_mel_tensor.unsqueeze(0))\n",
    "denoised_output = denoised_output.squeeze(0).squeeze(0).detach().numpy()\n",
    "denoised_output = librosa.db_to_power(denoised_output)\n",
    "\n",
    "audio_signal = librosa.feature.inverse.mel_to_audio(denoised_output, sr=sample_rate, n_iter=500)\n",
    "audio_signal = audio_signal / np.max(np.abs(audio_signal))\n",
    "\n",
    "librosa.display.waveshow(audio_signal, sr=sample_rate)\n",
    "soundfile.write('test_librosa_denoised.wav', audio_signal, sample_rate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
